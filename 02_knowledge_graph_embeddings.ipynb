{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HbemiIAs4VCC"
   },
   "source": [
    "# Knowledge Graph Embeddings\n",
    "\n",
    "Word embeddings aim at capturing the meaning of words based on very large corpora; however, there are decades of experience and approaches that have tried to capture this meaning by structuring knowledge into semantic nets, ontologies and graphs. \n",
    "\n",
    "|         | Neural           | Symbolic  |\n",
    "| ------------- |-------------| -----|\n",
    "| **representation**      | vectors | symbols (URIs) |\n",
    "| **input**               | large corpora   | human editors (Knowledge engineers) |\n",
    "| **interpretability**      | linked to model and training dataset      |   requires understanding of schema  |\n",
    "| **alignability**    | parallel (annotated) corpora | heuristics + manual |\n",
    "| **composability** | combine vectors | merge graphs | \n",
    "| **extensibility**   | fixed vocabulary | need to know how to link new nodes |\n",
    "| **certainty**        | fuzzy | exact |\n",
    "| **debugability**  | 'fix' training data? | edit graph |\n",
    "\n",
    "In recent years, many new approaches have been proposed to derive 'neural' representations for existing knowledge graphs. Think of this as trying to capture the knowledge encoded in the KG to make it easier to use this in deep learning models.\n",
    "\n",
    " - [TransE (2013)](http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf): try to assign an embedding to nodes and relations, so that $h + r$ is close to $t$, where $h$ and $t$ are nodes in the graph and $r$ is an edge. In the RDF world, this is simply an RDF triple where $h$ is the subject $r$ is the property and $t$ is the object of the triple.\n",
    " - [HolE (2016)](http://arxiv.org/abs/1510.04935): Variant of TransE, but uses a different operator (circular correlation) to represent pairs of entities.\n",
    " - [RDF2Vec(2016)](https://ub-madoc.bib.uni-mannheim.de/41307/1/Ristoski_RDF2Vec.pdf): applies word2vec to random walks on an RDF graph (essentially paths or sequences of nodes in the graph). \n",
    " - [Graph convolutions(2018)](http://arxiv.org/abs/1703.06103): apply convolutional operations on graphs to learn the embeddings.\n",
    " - [Neural message passing(2018)](https://arxiv.org/abs/1704.01212): merges two strands of research on KG embeddings: recurrent and convolutional approaches.\n",
    " \n",
    "For more background: [Nickel, M., Murphy, K., Tresp, V., & Gabrilovich, E. (2016). A review of relational machine learning for knowledge graphs. Proceedings of the IEEE, 104(1), 11â€“33. https://doi.org/10.1109/JPROC.2015.2483592](http://www.dbs.ifi.lmu.de/~tresp/papers/1503.00759v3.pdf) provides a good overview (up to 2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DHfn9HX6__Wt"
   },
   "source": [
    "# Creating embeddings for WordNet\n",
    "\n",
    "In this section, we go through the steps of generating word and concept embeddings using WordNet, a lexico-semantic knowledge graph.\n",
    "  \n",
    "  0. Choose (or implement) a KG embedding algorithm\n",
    "  1. Convert the KG into format required by embedding algorithm\n",
    "  2. Execute the training\n",
    "  3. Evaluate/inspect results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LB3dpgvCAC_U"
   },
   "source": [
    "## Choose embedding algorithm: HolE\n",
    "\n",
    "We will use an [existing implementation of the `HolE` algorithm available on GitHub](https://github.com/mnick/holographic-embeddings). \n",
    "\n",
    "### Install `scikit-kge`\n",
    "\n",
    "The `holographic-embeddings` repo is actually just a wrapper around `scikit-kge` or [SKGE](https://github.com/mnick/scikit-kge), a library that implements a few KG embedding algorithms. First, we need to install `scikit-kge` as a library in our environment. Execute the following cells to clone the repository and install the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-xScJEUjoM3C"
   },
   "outputs": [],
   "source": [
    "# make sure we are in the right folder to perform the git clone\n",
    "%cd /content/\n",
    "!git clone https://github.com/hybridNLP2018/scikit-kge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D-hV6dZN3RXF"
   },
   "outputs": [],
   "source": [
    "%cd scikit-kge\n",
    "# install a dependency of scikit-kge on the colaboratory environment, needed to correclty build scikit-kge\n",
    "!pip install nose\n",
    "# now build a source distribution for the project\n",
    "!python setup.py sdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EvE6LPvV3Xz7"
   },
   "source": [
    "Executing the previous cell should produce a lot of output as the project is built. Towards the end you should see something like:\n",
    "\n",
    "```\n",
    "Writing scikit-kge-0.1/setup.cfg\n",
    "creating dist\n",
    "Creating tar archive\n",
    "```\n",
    "\n",
    "This should have created a `tar.gz` file in the `dist` subfolder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2yS1Ny4U6Vr"
   },
   "outputs": [],
   "source": [
    "!ls dist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pRrluMqDWbrK"
   },
   "source": [
    "which we can install on the local environment by using `pip`, the python package manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1g6TPorMV0dv"
   },
   "outputs": [],
   "source": [
    "!pip install dist/scikit-kge-0.1.tar.gz\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WizYMgrc4rTJ"
   },
   "source": [
    "### Install and inspect `holographic_embeddings` repo\n",
    "Now that `skge` is installed on this environment, we are ready to clone the [holographic-embeddings](https://github.com/mnick/holographic-embeddings) repository, which will enable us to train `HolE` embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VSZaNq4pFr1I"
   },
   "outputs": [],
   "source": [
    "# let's go back to the main \\content folder and clone the holE repo\n",
    "%cd /content/\n",
    "!git clone https://github.com/mnick/holographic-embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d1Yoa6zR6mpA"
   },
   "source": [
    "If you want, you can browse the contents of this repo on github, or execute the following to see how you can start training embeddings for the WordNet 1.8 knowledge graph. In the following sections we'll go into more detail about how to train embeddings, so there is no need to actually execute this training just yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E9nlPWCHO1cW"
   },
   "outputs": [],
   "source": [
    "%less holographic-embeddings/run_hole_wn18.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qeE8XuMM7p4x"
   },
   "source": [
    "You should see a section on the bottom of the screen with the contents of the `run_hole_wn18.sh` file. The main execution is:\n",
    "\n",
    "```\n",
    "python kg/run_hole.py --fin data/wn18.bin \\\n",
    "       --test-all 50 --nb 100 --me 500 \\\n",
    "       --margin 0.2 --lr 0.1 --ncomp 150\n",
    "```\n",
    "\n",
    "which is just executing the `kg/run_hole.py` script on the input data `data/wn18.bin` and passing various arguments to control how to train and produce the embeddings:\n",
    "\n",
    "  * `me`: states the number of epochs to train for (i.e. number of times to go through the input dataset)\n",
    "  * `ncomp`: specifies the dimension of the embeddings, each embedding will be a vector of 150 dimensions\n",
    "  * `nb`: number of batches\n",
    "  * `test-all`: specifies how often to run validation of the intermediate embeddings. In this case, every 50 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cmb4X3kFC79i"
   },
   "source": [
    "## Convert WordNet KG to required input\n",
    "### KG Input format required by SKGE\n",
    "SKGE requires a graph to be represented as a serialized python dictionary with the following structure:\n",
    "  * `relations`: a list of relation names (the named edges in the graph)\n",
    "  * `entities`:  a list of entity names (the nodes in the graph), \n",
    "  * `train_subs`: a list of triples of the form `(head_id, tail_id, rel_id)`, where `head_id` and `tail_id` refer to the index in the `entities`list and `rel_id` refers to the index in the `relations` list. This is the list of triples that will be used to train the embeddings.\n",
    "  * `valid_subs`: a list of triples of the same form as `train_subs`. These are used to validate the embeddings during training (and thus to tune hyperparameters).\n",
    "  * `test_subs`: a list of triples of the same form as `test_subs`.  These are used to test the learned embeddings.\n",
    "\n",
    "The `holographic-embeddings` GitHub repo comes with an example input file: `data/wn18.bin` for WordNet 1.8. In the following executable cell, we show how to read and inspect data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DwVgR4aIcw7T"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "with open('holographic-embeddings/data/wn18.bin', 'rb') as fin:\n",
    "  wn18_data = pickle.load(fin)\n",
    "\n",
    "for k in wn18_data:\n",
    "  print(k, type(wn18_data[k]), len(wn18_data[k]), wn18_data[k][-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UFdc9bhm97uL"
   },
   "source": [
    "The expected output should be similar to:\n",
    "\n",
    "```\n",
    "relations <class 'list'> 18 ['_synset_domain_region_of', '_verb_group', '_similar_to']\n",
    "train_subs <class 'list'> 141442 [(5395, 37068, 9), (5439, 35322, 11), (28914, 1188, 10)]\n",
    "entities <class 'list'> 40943 ['01164618', '02371344', '03788703']\n",
    "test_subs <class 'list'> 5000 [(17206, 33576, 0), (1179, 11861, 0), (30287, 1443, 1)]\n",
    "valid_subs <class 'list'> 5000 [(351, 25434, 0), (3951, 2114, 7), (756, 14490, 0)]\n",
    "```\n",
    "This shows that WordNet 1.8 has been represented as a graph of 40943 nodes (which we assume correspond to the synsets) interlinked using 18 relation types. The full set of relations has been split into 141K triples for training, and 5K triples each for testing and validation. \n",
    "\n",
    "### Converting WordNet 3.0 into the required input format\n",
    "WordNet 1.8 is a bit dated and it will be useful to have experience converting your KG into the required input format. Hence, rather than simply reusing the `wn18.bin` input file, we will generate our own directly from the [NLTK WordNet API](http://www.nltk.org/howto/wordnet.html).\n",
    "\n",
    "First we need to download WordNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_tFy4avjAavZ"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ufzFysmIA-6k"
   },
   "source": [
    "#### Explore WordNet API\n",
    "Now that we have the KG, we can use the WordNet API to explore the graph. Refer to the [howto doc](http://www.nltk.org/howto/wordnet.html) for a more in depth overview, here we only show a few methods that will be needed to generate our input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M3QyMddMBjJG"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Clgt9N15BqOu"
   },
   "source": [
    "The main nodes in WordNet are called synsets (synonym sets). These correspond roughly to *concepts*. You can find all the synstes related to a word like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XEwlT4Q5Blvf"
   },
   "outputs": [],
   "source": [
    "wn.synsets('dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_1GggpiCI1o"
   },
   "source": [
    "The output from the cell above shows how synsets are identified by the NLTK WordNet API. They have the form `<main-lemma>.<POS-code>.<sense-number>`. As far as we are aware, this is a format chosen by the implementors of the NLTK WordNet API and other APIs may choose diverging ways to refer to synsets. \n",
    "\n",
    "You can get a list of all the synsets as follows (we only show the first 5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JwVHDCLFCIVq"
   },
   "outputs": [],
   "source": [
    "for synset in list(wn.all_synsets())[:5]:\n",
    "    print(synset.name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E7IoiUDvDm2T"
   },
   "source": [
    "Similarly, you can also get a list of all the lemma names (again we only show 5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fadVvW4sDmfO"
   },
   "outputs": [],
   "source": [
    "for lemma in list(wn.all_lemma_names())[5000:5005]:\n",
    "    print(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p1JmAO4GGIVd"
   },
   "source": [
    "For a given synset, you can find related synsets or lemmas, by calling the functions for each relation type. Below we provide a couple of examples for the first sense of adjective *adaxial*. In the first example, we see that this synset belongs to  `topic domain` `biology.n.01`, which is again a synset. In the second example, we see that it has two lemmas, which are relative to the synset. In the third example, we retrieve the lemmas in a form that are not relative to the synset, which is the one we will use later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvN_hL05GHhV"
   },
   "outputs": [],
   "source": [
    "wn.synset('adaxial.a.01').topic_domains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vd3bxAtQHpog"
   },
   "outputs": [],
   "source": [
    "wn.synset('adaxial.a.01').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9jE9OJ97IolL"
   },
   "outputs": [],
   "source": [
    "wn.synset('adaxial.a.01').lemma_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f14znvu9D8vy"
   },
   "source": [
    "#### Entities and relations to include\n",
    "\n",
    "The main nodes in WordNet are the syncons, however, lemmas can also be considered to be nodes in the graph. Hence, you need to decide which nodes to include. Since we are interested in capturing as much information as can be provided by WordNet, we will include both synsets and lemmas.\n",
    "\n",
    "WordNet defines a large number of relations between synsets and lemmas. Again, you can decide to include all or just some of these. One particularity of WordNet is that many relations are defined twice: e.g. hypernym and hyponym are the exact same relation, but in reverse order. Since this is not really providing additional information, we only include such relations once. The following cell defines all the relations we will be taking into account. We represent these as python dictionaries, where the keys are the name of the relation and the values are functions that accept a `head` entity and produce a list of `tail` entities for that specific relation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fWxLDcSPFvIt"
   },
   "outputs": [],
   "source": [
    "syn_relations = {\n",
    "    'hyponym': lambda syn: syn.hyponyms(), \n",
    "    'instance_hyponym': lambda syn: syn.instance_hyponyms(),  \n",
    "    'member_meronym': lambda syn: syn.member_meronyms(),\n",
    "    'has_part': lambda syn: syn.part_meronyms(), \n",
    "    'topic_domain': lambda syn: syn.topic_domains(), \n",
    "    'usage_domain': lambda syn: syn.usage_domains(), \n",
    "    '_member_of_domain_region': lambda syn: syn.region_domains(),\n",
    "    'attribute': lambda syn: syn.attributes(),\n",
    "    'entailment': lambda syn: syn.entailments(),\n",
    "    'cause': lambda syn: syn.causes(),\n",
    "    'also_see': lambda syn: syn.also_sees(),\n",
    "    'verb_group': lambda syn: syn.verb_groups(),\n",
    "    'similar_to': lambda syn: syn.similar_tos()\n",
    "}\n",
    "lem_relations = {\n",
    "    'antonym': lambda lem: lem.antonyms(),\n",
    "    'derivationally_related_form': lambda lem: lem.derivationally_related_forms(),\n",
    "    'pertainym': lambda lem: lem.pertainyms()\n",
    "}\n",
    "\n",
    "syn2lem_relations = {\n",
    "    'lemma': lambda syn: syn.lemma_names()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GcnuKwnPJneC"
   },
   "source": [
    "#### Triple generation\n",
    "\n",
    "We are now ready to generate triples by using the WordNet API. Recall that `skge` requires triples of the form `(head_id, tail_id, rel_id)`, hence we will need to have some way of mapping entity (synset and lemma) names and relations types to  unique ids. We therefore assume we will have an `entity_id_map` and a `rel_id_map`, which will map the entity name (or relation type) to an id. The following two cells implement functions which will iterate through the synsets and relations to generate the triples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jFkC4hMRLnvc"
   },
   "outputs": [],
   "source": [
    "def generate_syn_triples(entity_id_map, rel_id_map):\n",
    "  result = []\n",
    "  for synset in list(wn.all_synsets()):\n",
    "    h_id = entity_id_map.get(synset.name())\n",
    "    if h_id is None:\n",
    "      print('No entity id for ', synset)\n",
    "      continue\n",
    "    for synrel, srfn in syn_relations.items():\n",
    "      r_id = rel_id_map.get(synrel)\n",
    "      if r_id is None:\n",
    "        print('No rel id for', synrel)\n",
    "        continue\n",
    "      for obj in srfn(synset):  \n",
    "        t_id = entity_id_map.get(obj.name())\n",
    "        if t_id is None:\n",
    "          print('No entity id for object', obj)\n",
    "          continue\n",
    "        result.append((h_id, t_id, r_id))\n",
    "    \n",
    "    for rel, fn in syn2lem_relations.items():\n",
    "      r_id = rel_id_map.get(rel)\n",
    "      if r_id is None:\n",
    "        print('No rel id for', rel)\n",
    "        continue\n",
    "      for obj in fn(synset):\n",
    "        lem = obj.lower()\n",
    "        t_id = entity_id_map.get(lem)\n",
    "        if t_id is None:\n",
    "          print('No entity id for object', obj, 'lowercased:', lem)\n",
    "          continue\n",
    "        result.append((h_id, t_id, r_id))\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVR1Qb27LrBV"
   },
   "outputs": [],
   "source": [
    "def generate_lem_triples(entity_id_map, rel_id_map):\n",
    "  result = []\n",
    "  for lemma in list(wn.all_lemma_names()):\n",
    "    h_id = entity_id_map.get(lemma)\n",
    "    if h_id is None:\n",
    "      print('No entity id for lemma', lemma)\n",
    "      continue\n",
    "    _lems = wn.lemmas(lemma)\n",
    "    for lemrel, lrfn in lem_relations.items():\n",
    "      r_id = rel_id_map.get(lemrel)\n",
    "      if r_id is None:\n",
    "        print('No rel id for ', lemrel)\n",
    "        continue\n",
    "      for _lem in _lems:\n",
    "        for obj in lrfn(_lem):\n",
    "          t_id = entity_id_map.get(obj.name().lower())\n",
    "          if t_id is None:\n",
    "            print('No entity id for obj lemma', obj, obj.name())\n",
    "            continue\n",
    "          result.append((h_id, t_id, r_id))\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f604qgU6M_WV"
   },
   "source": [
    "#### Putting it all together\n",
    "Now that we have methods for generating lists of triples, we can generate the input dictionary and serialise it. We need to:\n",
    "  * create our lists of entities and relations, \n",
    "  * derive a map from entity and relation names to ids\n",
    "  * generate the triples\n",
    "  * split the triples into training, validation and test subsets\n",
    "  * write the python dict to a serialised file\n",
    "  \n",
    "We implement this in the following method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QwBOWdRVM-EA"
   },
   "outputs": [],
   "source": [
    "import random # for shuffling list of triples\n",
    "      \n",
    "def wnet30_holE_bin(out):\n",
    "  \"\"\"Creates a skge-compatible bin file for training HolE embeddings based on WordNet31\"\"\"\n",
    "  synsets = [synset.name() for synset in wn.all_synsets()]\n",
    "  lemmas = [lemma for lemma in wn.all_lemma_names()]\n",
    "  entities = list(synsets + list(set(lemmas)))\n",
    "  print('Found %s synsets, %s lemmas, hence %s entities' % (len(synsets), len(lemmas), len(entities)))\n",
    "  entity_id_map = {ent_name: id for id, ent_name in enumerate(entities)}\n",
    "  n_entity = len(entity_id_map)\n",
    "    \n",
    "  print(\"N_ENTITY: %d\" % n_entity)\n",
    "    \n",
    "  relations = list( list(syn_relations.keys()) + list(lem_relations.keys()) + list(syn2lem_relations.keys()))\n",
    "  relation_id_map = {rel_name: id for id, rel_name in enumerate(relations)}\n",
    "  n_rel = len(relation_id_map)\n",
    "    \n",
    "  print(\"N_REL: %d\" % n_rel)\n",
    "  print('relations', relation_id_map)\n",
    "    \n",
    "  syn_triples = generate_syn_triples(entity_id_map, relation_id_map)\n",
    "  print(\"Syn2syn relations\", len(syn_triples))\n",
    "  lem_triples = generate_lem_triples(entity_id_map, relation_id_map)\n",
    "  print(\"Lem2lem relations\", len(lem_triples))\n",
    "  all_triples = syn_triples + lem_triples\n",
    "  print(\"All triples\", len(all_triples))\n",
    "  random.shuffle(all_triples)\n",
    "    \n",
    "  test_triple = all_triples[:500]\n",
    "  valid_triple = all_triples[500:1000]\n",
    "  train_triple = all_triples[1000:]\n",
    "        \n",
    "  to_pickle = {\n",
    "      \"entities\": entities,\n",
    "      \"relations\": relations,\n",
    "      \"train_subs\": train_triple,\n",
    "      \"test_subs\": test_triple,\n",
    "      \"valid_subs\": valid_triple\n",
    "  }\n",
    "    \n",
    "  with open(out, 'wb') as handle:\n",
    "    pickle.dump(to_pickle, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "  print(\"wrote to %s\" % out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JGkpEF3-O9qH"
   },
   "source": [
    "#### Generate `wn30.bin`\n",
    "Now we are ready to generate the  `wn30.bin` file which we can feed to the `HolE` algorithm implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aZ8tLDQPPPL7"
   },
   "outputs": [],
   "source": [
    "out_bin='/content/holographic-embeddings/data/wn30.bin'\n",
    "wnet30_holE_bin(out_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IyhyacPaRKil"
   },
   "source": [
    "Notice, that the resulting dataset now contains 265K entities, compared to 41K in WordNet 1.8 (to be fair, only 118K of the entities are synsets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X8MCByxEDHpE"
   },
   "source": [
    "## Learn the embeddings\n",
    "Now, we will use the WordNet 3.0 dataset to learn embeddings for both synsets and lemmas. Since this is fairly slow, we only train for 2 epochs, which can take up to 10 minutes (In the exercises at the end of this notebook, we provide a link to download pre-computed embeddings which have been trained for 500 epochs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QDMTf-8ISm2E"
   },
   "outputs": [],
   "source": [
    "wn30_holE_out='/content/wn30_holE_2e.bin'\n",
    "holE_dim=150\n",
    "num_epochs=2\n",
    "!python /content/holographic-embeddings/kg/run_hole.py --fin {out_bin} --fout {wn30_holE_out} \\\n",
    "  --nb 100 --me {num_epochs} --margin 0.2 --lr 0.1 --ncomp {holE_dim}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qtd6MVfqrCF3"
   },
   "source": [
    "The output should look similar to:\n",
    "```\n",
    "INFO:EX-KG:Fitting model HolE with trainer PairwiseStochasticTrainer and parameters Namespace(afs='sigmoid', fin='/content/holographic-embeddings/data/wn30.bin', fout='/content/wn30_holE_2e.bin', init='nunif', lr=0.1, margin=0.2, me=2, mode='rank', nb=100, ncomp=150, ne=1, no_pairwise=False, rparam=0, sampler='random-mode', test_all=10)\n",
    "INFO:EX-KG:[  1] time = 120s, violations = 773683\n",
    "INFO:EX-KG:[  2] time = 73s, violations = 334894\n",
    "INFO:EX-KG:[  2] time = 73s, violations = 334894\n",
    "INFO:EX-KG:[  2] VALID: MRR = 0.11/0.12, Mean Rank = 90012.28/90006.14, Hits@10 = 15.02/15.12\n",
    "DEBUG:EX-KG:FMRR valid = 0.122450, best = -1.000000\n",
    "INFO:EX-KG:[  2] TEST: MRR = 0.11/0.12, Mean Rank = 95344.42/95335.96, Hits@10 = 15.74/15.74\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UoSer5UNDNBN"
   },
   "source": [
    "## Inspect resulting embeddings\n",
    "\n",
    "Now that we have trained the model, we can retrieve the embeddings for the entities and inspect them. \n",
    "\n",
    "### `skge` output file format\n",
    "The output file is again a pickled serialisation of a python dictionary. It contains the `model` itself, and results for the test and validation runs as well as execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9aI7ct7J_skp"
   },
   "outputs": [],
   "source": [
    "with open(wn30_holE_out, 'rb') as fin:\n",
    "    hole_model = pickle.load(fin)\n",
    "print(type(hole_model), len(hole_model))\n",
    "for k in hole_model:\n",
    "    print(k, type(hole_model[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehQ13ay2Aayi"
   },
   "source": [
    "We are interested in the model itself, which is an instance of a `skge.hole.HolE` class and has various parameters. The entity embeddings are stored in parameter `E`, which is essentially a matrix of $n_e \\times d$, where $n_e$ is the number of entities and $d$ is the dimension of each vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBwdXeTfAwsC"
   },
   "outputs": [],
   "source": [
    "model = hole_model['model']\n",
    "E = model.params['E']\n",
    "print(type(E), E.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQhosG_1G7fc"
   },
   "source": [
    "### Converting embeddings to more inspectable format\n",
    "Unfortunately, `skge` does not provide methods for exploring the embedding space. (KG embedding libraries are more geared towards prediction of relations) So, we will convert the embeddings into an easier to explore format. We first convert them into a pair of files for the vectors and the vocabulary and we will then use the `swivel` library to explore the results.\n",
    "\n",
    "We first read the list of entities, this is our **vocabulary** (i.e. names of synsets and lemmas for which we have embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gjNzfDrUBQ4u"
   },
   "outputs": [],
   "source": [
    "with open('/content/holographic-embeddings/data/wn30.bin', 'rb') as fin:\n",
    "  wn30_data = pickle.load(fin)\n",
    "entities = wn30_data['entities']\n",
    "len(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C1LXP4wkKrqU"
   },
   "source": [
    "Next, we generate a vocab file and a `tsv` file where each line contains the word and a list of $d$ numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rWQxUz5UB9Me"
   },
   "outputs": [],
   "source": [
    "vec_file = '/content/wn30_holE_2e.tsv'\n",
    "vocab_file = '/content/wn30_holE_2e.vocab.txt'\n",
    "\n",
    "with open(vocab_file, 'w', encoding='utf_8') as f:\n",
    "  for i, w in enumerate(entities):\n",
    "    word = w.strip()\n",
    "    print(word, file=f)\n",
    "    \n",
    "with open(vec_file, 'w', encoding='utf_8') as f:\n",
    "  for i, w in enumerate(entities):\n",
    "    word = w.strip()\n",
    "    embedding = E[i]\n",
    "    print('\\t'.join([word] + [str(x) for x in embedding]), file=f)\n",
    "!wc -l {vec_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0UowH1KnK9cH"
   },
   "source": [
    "Now that we have these files, we can use `swivel`, which we used in the first notebook to inspect the embeddings.\n",
    "\n",
    "#### Download tutorial materials and `swivel` (if necessary)\n",
    "Download swivel, although you may already have it on your environment if you already executed the first notebook of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJrWIbhwF80o"
   },
   "outputs": [],
   "source": [
    "%cd /content\n",
    "!git clone https://github.com/hybridnlp/tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ba8CASiOP8Uq"
   },
   "source": [
    "Use the  `swivel/text2bin` script to convert the `tsv` embeddings into `swivel`'s binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P514VWNQHq_s"
   },
   "outputs": [],
   "source": [
    "vecbin = '/content/wn30_holE_2e.tsv.bin'\n",
    "!python /content/tutorial/scripts/swivel/text2bin.py --vocab={vocab_file} --output={vecbin} \\\n",
    "        {vec_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FV0fJvl0QNJD"
   },
   "source": [
    "Next, we can load the vectors using `swivel`'s `Vecs` class, which provides easy inspection of neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NSfJruwIKb9"
   },
   "outputs": [],
   "source": [
    "from tutorial.scripts.swivel import vecs\n",
    "vectors = vecs.Vecs(vocab_file, vecbin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w7oJwdTBQhl6"
   },
   "source": [
    "#### Inspect a few example lemmas and synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0R385FZ9IofP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(vectors.k_neighbors('california'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gn3de2faJUap"
   },
   "outputs": [],
   "source": [
    "wn.synsets('california')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gjToHhVuJamz"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(vectors.k_neighbors('california.n.01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KzMtPwTjJKF-"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(vectors.k_neighbors('conference'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JjKHy8t6JNOA"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(vectors.k_neighbors('semantic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "THbN3gjuJsc7"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(vectors.k_neighbors('semantic.a.01'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hMFracTidgx6"
   },
   "source": [
    "As you can see, the embeddings do not look very good at the moment. In part this is due to the fact we only trained the model for 2 epochs. We have pre-calculated a set of HolE embeddings for 500 epochs, which you can download and inspect as part of an optional excercise below. Results for these are much better:\n",
    "\n",
    "|    cosine sim    | entity  |\n",
    "| ------------- |-------------|\n",
    "| 1.0000 | lem_california |\n",
    "| 0.4676 | lem_golden_state |\n",
    "| 0.4327 | lem_ca |\n",
    "| 0.4004 | lem_californian |\n",
    "| 0.3838 | lem_calif. |\n",
    "| 0.3500 | lem_fade |\n",
    "| 0.3419 | lem_keystone_state |\n",
    "| 0.3375 | wn31_antilles.n.01 |\n",
    "| 0.3356 | wn31_austronesia.n.01 |\n",
    "| 0.3340 | wn31_overbalance.v.02 |\n",
    "\n",
    "For the synset for california, we also see 'sensible' results:\n",
    "\n",
    "|    cosine sim    | entity  |\n",
    "| ------------- |-------------|\n",
    "| 1.0000 | wn31_california.n.01 |\n",
    "| 0.4909 | wn31_nevada.n.01 |\n",
    "| 0.4673 | wn31_arizona.n.01 |\n",
    "| 0.4593 | wn31_tennessee.n.01 |\n",
    "| 0.4587 | wn31_new_hampshire.n.01 |\n",
    "| 0.4555 | wn31_sierra_nevada.n.02 |\n",
    "| 0.4073 | wn31_georgia.n.01 |\n",
    "| 0.4048 | wn31_west_virginia.n.01 |\n",
    "| 0.3991| wn31_north_carolina.n.01 |\n",
    "| 0.3977 | wn31_virginia.n.01 |\n",
    "\n",
    "One thing to notice here is that all of the top 10 closely related entities for `california.n.01` are also synsets. Similarly for lemma `california`, the most closely related entities are also lemmas, although some synsets also made it into the top 10 neighbours. This may indicate a tendency of `HolE` to keep lemmas close to other lemmas and synsets close to other synsets. In general, choices about how nodes in the KG are related will affect how their embeddings are interrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HKGIYZs0DSiF"
   },
   "source": [
    "# Conclusion and exercises\n",
    "\n",
    "In this notebook we provided an overview of recent knowledge graph embedding approaches and showed how to use existing implementations to generate word and concept embeddings for WordNet 3.0. \n",
    "\n",
    "## Excercise: train embeddings on your own KG\n",
    "If you have a KG of your own, you can adapt the code shown above to generate a graph representation as expected by `skge` and you can train your embeddings in this way. Popular KGs are Freebase and DBpedia.\n",
    "\n",
    "## Excercise: inspect embeddings for pre-calculated WordNet 3.0\n",
    "We have used code similar to the one shown above to train embeddings for 500 epochs using HolE. You can execute the following cells to download and explore these embeddings. The embeddings are about 142MB, so dowloading them may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4AdyyqHV_4SU"
   },
   "outputs": [],
   "source": [
    "!mkdir /content/vec/\n",
    "%cd /content/vec/\n",
    "!wget https://zenodo.org/record/1446214/files/wn-en-3.0-HolE-500e-150d.tar.gz\n",
    "!tar -xzf wn-en-3.0-HolE-500e-150d.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Dplzenmc56f"
   },
   "outputs": [],
   "source": [
    "%ls /content/vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aL1wkGRjhQ2P"
   },
   "source": [
    "The downloaded tar contains a `tsv.bin` and a `vocab` file like the one we created above. We can use it to load the vectors using `swivel`'s `Vecs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ISMxP7tRc7QR"
   },
   "outputs": [],
   "source": [
    "vocab_file = '/content/vec/wn-en-3.1-HolE-500e.vocab.txt'\n",
    "vecbin = '/content/vec/wn-en-3.1-HolE-500e.tsv.bin'\n",
    "wnHolE = vecs.Vecs(vocab_file, vecbin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y_0wm8Pfhfqg"
   },
   "source": [
    "Now you are ready to start exploring. The only thing to notice is that we have added a prefix to `lem_` to all lemmas and `wn31_` to all synsets, as shown in the following examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-DTNIKSPdKJS"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(wnHolE.k_neighbors('lem_california'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7qGuxC4ldNT6"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(wnHolE.k_neighbors('wn31_california.n.01'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_knowledge_graph_embeddings.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
