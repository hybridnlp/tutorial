{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01a_nlm_and_contextual_embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "77f1ac19effb4dbab5e416acf5617b7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e602650934884f489e0eabcb97343c7a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6fa65b848bd54ec58935d0fbe9da8c42",
              "IPY_MODEL_58fae47344ff4ebd983a518fb3f59f53"
            ]
          }
        },
        "e602650934884f489e0eabcb97343c7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6fa65b848bd54ec58935d0fbe9da8c42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c6078fa530d942ba9431f6076a4c698e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_71c976da80934f05be46e382815d3b1f"
          }
        },
        "58fae47344ff4ebd983a518fb3f59f53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_39260f42789f46e3926aa159e3dbb303",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 2.52MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_43c1896c2b7a4201a9384a7830dcc92b"
          }
        },
        "c6078fa530d942ba9431f6076a4c698e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "71c976da80934f05be46e382815d3b1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "39260f42789f46e3926aa159e3dbb303": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "43c1896c2b7a4201a9384a7830dcc92b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3VSgZXfFdCk",
        "colab_type": "text"
      },
      "source": [
        "# Fine tuning pre-trained language models for text classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzrX_0SDViOu",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "Fine-tuning pre-trained language models learnt with transformers has improved the state of the art in multiple NLP evaluation tasks(see [SuperGlue leader board](https://super.gluebenchmark.com/leaderboard)). Learning a language model is an unsupervised task where the model learns to predict the next word in a sequence given the previous words. Neural language models have been implemented as feed foorward networks, LSTMS (ELMo, ULMFit), and transformers-encoders (BERT) or decoders (Open AI GPT).\n",
        "\n",
        "In this notebook we fine tune a BERT pre-trained language model to carry out a binary classification task where tweets are labelled as generated by bots or hurmans. \n",
        "\n",
        "<!-- The notebook is structured as follows:\n",
        "\n",
        "- Motivation\n",
        "- Setup\n",
        "  - Libraries required\n",
        "  - Dataset\n",
        "- A glimpse on BERT tokenization \n",
        "- Fine tune the model\n",
        "- Evaluate the BERT classifier \n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUlcUwnALTCn",
        "colab_type": "text"
      },
      "source": [
        "## Motivation\n",
        "**While word embeddings are learnt from large corpora, their use in neural models to solve specific tasks is limited to the input layer.** So in practice a task-specific neural model is built almost from scratch because most of the model parameters are initialized randomly, and hence, these paremeters need to be optimized for the task at hand, requiring large sets of data to produce a high performance model.\n",
        "\n",
        "**Recent advances in neural language models** (BERT or OPEN AI GPT) have shown evidence that task specific architectures are not longer necessary and transfering some internal representations (attention blocks) along with shallow feed forward networks is enough. \n",
        "\n",
        "**In (Garcia et al.,2019) we presented an experimental study** on the use of word embeddings as input of CNN architectures and Bi-LSTM to tackle the bot detection task and compare these results with fine-tuning pretrained language models. \n",
        "\n",
        "**Evaluation results, presented in the figure below, show that fine-tuning language models yields overall better results than training specific neural architectures** that are fed with mixture of: i) pre-trained contextualized word embeddings (ELMo), ii) pre-trained  context-indepedent word embeddings learnt from Common Crawl(FastText), Twitter (GloVe), and urban dictionary (word2vec), plus embeddings optimized by the neural network in the learning process. \n",
        "\n",
        "![Bot detection classification task](https://drive.google.com/uc?id=1rSzM544MK2QOezpvUKHfrxATbkEiyBHX)\n",
        "\n",
        "\n",
        "\n",
        "**References**\n",
        "\n",
        "Garcia-Silva, Andres, et al. \"An Empirical Study on Pre-trained Embeddings and Language Models for Bot Detection.\" Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019). 2019.\n",
        "\n",
        "To cite this paper use the following BibTex entry: \n",
        "\n",
        "```\n",
        "@inproceedings{garcia-silva-etal-2019-empirical,\n",
        "    title = \"An Empirical Study on Pre-trained Embeddings and Language Models for Bot Detection\",\n",
        "    author = \"Garcia-Silva, Andres  and\n",
        "      Berrio, Cristian  and\n",
        "      G{\\'o}mez-P{\\'e}rez, Jos{\\'e} Manuel\",\n",
        "    booktitle = \"Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)\",\n",
        "    month = aug,\n",
        "    year = \"2019\",\n",
        "    address = \"Florence, Italy\",\n",
        "    publisher = \"Association for Computational Linguistics\",\n",
        "    url = \"https://www.aclweb.org/anthology/W19-4317\",\n",
        "    doi = \"10.18653/v1/W19-4317\",\n",
        "    pages = \"148--155\",\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POOq3acQMrKT",
        "colab_type": "text"
      },
      "source": [
        "# A glimpse on BERT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U6jzPnupzRr",
        "colab_type": "text"
      },
      "source": [
        "## Input representation\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1tZS7sszhNtT3m25EZJkjC9PjRZDEPKGy\" alt=\"Token embeddings, segment embeddings, and positional embeddings\" width=\"500\"/>\n",
        "\n",
        "Image source: \"Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta8RVpIpUiyf",
        "colab_type": "text"
      },
      "source": [
        "## Pre-training learning objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRcyktIVwba8",
        "colab_type": "text"
      },
      "source": [
        "### Language Model\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1-17pLKqo6BqXbu7GerX1y_e9kJyghjNG\" alt=\"Language modeling objective\" width=\"500\"/>\n",
        "\n",
        "Image source: https://mlexplained.com/2019/06/30/paper-dissected-xlnet-generalized-autoregressive-pretraining-for-language-understanding-explained/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU-qjZNfODqh",
        "colab_type": "text"
      },
      "source": [
        "### BERT Learning Objective: Masked language model\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1q6C-6ont7lsuoyw4c1N4-GRFhcko0c4m\" alt=\"Masked LM\" width=\"500\"/>\n",
        "\n",
        "Image source: https://mlexplained.com/2019/06/30/paper-dissected-xlnet-generalized-autoregressive-pretraining-for-language-understanding-explained/\n",
        "\n",
        "### Bert: Transformer encoder\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=11GJiHlDeoKsShOwSJvMTq3fc8E8GxV4V\" alt=\"Masked LM\" width=\"500\"/>\n",
        "\n",
        "Image Source: https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPqLDsHCQh6U",
        "colab_type": "text"
      },
      "source": [
        "### Next Sentence Prediction\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1x0ckvgMwb5j3SfVNId-EyQgyDlt3C42h\" alt=\"next sentence prediction\" width=\"700\"/>\n",
        "\n",
        "Image source: http://jalammar.github.io/illustrated-bert/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pt2r_VzV1AS",
        "colab_type": "text"
      },
      "source": [
        "## Contextualized word embeddings\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1LozTCltkxXbkrE2M72r0lDUWEDXLEWZU\" alt=\"Contextualized embeddings\" width=\"600\"/>\n",
        "\n",
        "Image source: http://jalammar.github.io/illustrated-bert/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKABLAhkkcNW",
        "colab_type": "text"
      },
      "source": [
        "## Attention mechanism\n",
        "\n",
        "https://drive.google.com/open?id=1-8VYAUkC30yaQ65yr-moLmJmZmK1D3Xg\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1-8VYAUkC30yaQ65yr-moLmJmZmK1D3Xg\" alt=\"Attention Mechanism\" width=\"500\"/>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtN3rqyOOEbB",
        "colab_type": "text"
      },
      "source": [
        "## Fine-tuning: Supervised Training on a specific task\n",
        "\n",
        "fine-tuning BERT for a task just requires to incorporate one  additional  output  layer,  so a minimal number of parameters need to be learned from scratch.\n",
        "\n",
        "In the figure below E represents the input embedding,Ti represents the contextual representation of token i, [CLS] is the special symbol for classification output, and [SEP] is the special symbol to separate non-consecutive token sequences\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1wZPwbMNtHwf8g-7phWxwtJCxnTfPj-Ux\" width=\"600\"/>\n",
        "\n",
        "Image source: Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\n",
        "\n",
        "To **fine-tune BERT for a sequence classification task** the transfomer output for the CLS token is used as the sequence representation. The transfomer output for the CLS token is connected to a one layer feed forward network that predicts the classification labels. All the BERT parameters and the FF network are fine-tune jointly to maximize the log-probability of the correct label.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HR3iIvgVmnO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Experimental setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TZEolAIcM9_",
        "colab_type": "text"
      },
      "source": [
        "## Transformers library\n",
        "\n",
        "We use transformer from Huggingface: https://github.com/huggingface/transformers\n",
        "\n",
        "\"Transformers provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGBVqmkNqD_z",
        "colab_type": "code",
        "outputId": "84794989-2a85-4408-8df8-17ff64c3685a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/97/7db72a0beef1825f82188a4b923e62a146271ac2ced7928baa4d47ef2467/transformers-2.9.1-py3-none-any.whl (641kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 40.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 31.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 37.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=2a7fa73c5880f4b547598161ee533f23e623eb20a79da8faab18839478b62401\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAB8VCbxkjr4",
        "colab_type": "text"
      },
      "source": [
        "## Dataset \n",
        "\n",
        "**We use the bot detection dataset generated in (Garcia et al.,2019)** that was built starting from an existing list of twitter accounts that were manually labelled as bots and humans. Then we use the twitter API to extract tweets from these account.  In total the dataset contains around 600K tweet, approximately half of them generated by bots, and the other half by humans. \n",
        "\n",
        "In this notebook we provide a complete version of the dataset (large) and a reduced one (small) to be able to run the notebook whithin the time frame, since **fine tuning BERT on the large version takes more than 5h**. \n",
        "\n",
        "- Large: 500k train and 100k test labeled tweets which is in the path: \"'/content/gdrive/My Drive/09_BERT/Large_Dataset/\"\n",
        "- Small: 1k train and 100 test labeled tweets which is in the path: \"'/content/gdrive/My Drive/09_BERT/Small_Dataset/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XViapGYvR01H",
        "colab_type": "text"
      },
      "source": [
        "### Downloading from Google Drive\n",
        "\n",
        "Let's download the datasets and the models from Google Drive, and then decompress the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic91KXEt_pyf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "1c19c6f1-5357-4558-c48d-4d79bb3a2d27"
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1kq0NxztYDBBN_yWCnvGq_xtBo-bAResU' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1kq0NxztYDBBN_yWCnvGq_xtBo-bAResU\" -O BERT.tar && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-21 14:47:40--  https://docs.google.com/uc?export=download&confirm=AzeJ&id=1kq0NxztYDBBN_yWCnvGq_xtBo-bAResU\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.193.100, 172.217.193.101, 172.217.193.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.193.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-cc-docs.googleusercontent.com/docs/securesc/03gbe2qq1cal4slrk0s9cmqn0po8d2gu/ugs0ebc8d7jlralgrqmlf9b4ucuipldp/1590072450000/16197418968100245121/16455356080524975533Z/1kq0NxztYDBBN_yWCnvGq_xtBo-bAResU?e=download [following]\n",
            "--2020-05-21 14:47:40--  https://doc-00-cc-docs.googleusercontent.com/docs/securesc/03gbe2qq1cal4slrk0s9cmqn0po8d2gu/ugs0ebc8d7jlralgrqmlf9b4ucuipldp/1590072450000/16197418968100245121/16455356080524975533Z/1kq0NxztYDBBN_yWCnvGq_xtBo-bAResU?e=download\n",
            "Resolving doc-00-cc-docs.googleusercontent.com (doc-00-cc-docs.googleusercontent.com)... 108.177.13.132, 2607:f8b0:400c:c09::84\n",
            "Connecting to doc-00-cc-docs.googleusercontent.com (doc-00-cc-docs.googleusercontent.com)|108.177.13.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=5ins3u8qmnfac&continue=https://doc-00-cc-docs.googleusercontent.com/docs/securesc/03gbe2qq1cal4slrk0s9cmqn0po8d2gu/ugs0ebc8d7jlralgrqmlf9b4ucuipldp/1590072450000/16197418968100245121/16455356080524975533Z/1kq0NxztYDBBN_yWCnvGq_xtBo-bAResU?e%3Ddownload&hash=0toc09lsuhiq6fbr59lfl9f8plqrjikp [following]\n",
            "--2020-05-21 14:47:40--  https://docs.google.com/nonceSigner?nonce=5ins3u8qmnfac&continue=https://doc-00-cc-docs.googleusercontent.com/docs/securesc/03gbe2qq1cal4slrk0s9cmqn0po8d2gu/ugs0ebc8d7jlralgrqmlf9b4ucuipldp/1590072450000/16197418968100245121/16455356080524975533Z/1kq0NxztYDBBN_yWCnvGq_xtBo-bAResU?e%3Ddownload&hash=0toc09lsuhiq6fbr59lfl9f8plqrjikp\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.193.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-00-cc-docs.googleusercontent.com/docs/securesc/03gbe2qq1cal4slrk0s9cmqn0po8d2gu/ugs0ebc8d7jlralgrqmlf9b4ucuipldp/1590072450000/16197418968100245121/16455356080524975533Z/1kq0NxztYDBBN_yWCnvGq_xtBo-bAResU?e=download&nonce=5ins3u8qmnfac&user=16455356080524975533Z&hash=lad78jneq4retod7f078cg8rjjgejnes [following]\n",
            "--2020-05-21 14:47:40--  https://doc-00-cc-docs.googleusercontent.com/docs/securesc/03gbe2qq1cal4slrk0s9cmqn0po8d2gu/ugs0ebc8d7jlralgrqmlf9b4ucuipldp/1590072450000/16197418968100245121/16455356080524975533Z/1kq0NxztYDBBN_yWCnvGq_xtBo-bAResU?e=download&nonce=5ins3u8qmnfac&user=16455356080524975533Z&hash=lad78jneq4retod7f078cg8rjjgejnes\n",
            "Connecting to doc-00-cc-docs.googleusercontent.com (doc-00-cc-docs.googleusercontent.com)|108.177.13.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-tar]\n",
            "Saving to: ‘BERT.tar’\n",
            "\n",
            "BERT.tar                [            <=>     ] 811.04M   202MB/s    in 4.2s    \n",
            "\n",
            "2020-05-21 14:47:45 (191 MB/s) - ‘BERT.tar’ saved [850439480]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7WOVkEBks-n",
        "colab_type": "code",
        "outputId": "bd904dd9-d946-49ed-99aa-544c3535bbd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "!tar xzvf ./BERT.tar -C ."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "09_BERT/\n",
            "09_BERT/Bert_Classifier_Large/\n",
            "09_BERT/Bert_Classifier_Large/added_tokens.json\n",
            "09_BERT/Bert_Classifier_Large/config.json\n",
            "09_BERT/Bert_Classifier_Large/eval_results.txt\n",
            "09_BERT/Bert_Classifier_Large/predictions.txt\n",
            "09_BERT/Bert_Classifier_Large/pytorch_model.bin\n",
            "09_BERT/Bert_Classifier_Large/special_tokens_map.json\n",
            "09_BERT/Bert_Classifier_Large/tokenizer_config.json\n",
            "09_BERT/Bert_Classifier_Large/training_args.bin\n",
            "09_BERT/Bert_Classifier_Large/vocab.txt\n",
            "09_BERT/Large_Dataset/\n",
            "09_BERT/Large_Dataset/cached_dev_Bert_Classifier_Large_128_cola\n",
            "09_BERT/Large_Dataset/dev.tsv\n",
            "09_BERT/Large_Dataset/train.tsv\n",
            "09_BERT/run_glue.py\n",
            "09_BERT/Small_Dataset/\n",
            "09_BERT/Small_Dataset/Bert_Classifier/\n",
            "09_BERT/Small_Dataset/Bert_Classifier/added_tokens.json\n",
            "09_BERT/Small_Dataset/Bert_Classifier/config.json\n",
            "09_BERT/Small_Dataset/Bert_Classifier/eval_results.txt\n",
            "09_BERT/Small_Dataset/Bert_Classifier/predictions.txt\n",
            "09_BERT/Small_Dataset/Bert_Classifier/pytorch_model.bin\n",
            "09_BERT/Small_Dataset/Bert_Classifier/special_tokens_map.json\n",
            "09_BERT/Small_Dataset/Bert_Classifier/tokenizer_config.json\n",
            "09_BERT/Small_Dataset/Bert_Classifier/training_args.bin\n",
            "09_BERT/Small_Dataset/Bert_Classifier/vocab.txt\n",
            "09_BERT/Small_Dataset/cached_dev_bert-base-uncased_128_cola\n",
            "09_BERT/Small_Dataset/cached_train_bert-base-uncased_128_cola\n",
            "09_BERT/Small_Dataset/dev.tsv\n",
            "09_BERT/Small_Dataset/train.tsv\n",
            "09_BERT/Test_Dataset/\n",
            "09_BERT/Test_Dataset/cached_dev_Bert_Classifier_128_cola\n",
            "09_BERT/Test_Dataset/cached_dev_Bert_Classifier_Large_128_cola\n",
            "09_BERT/Test_Dataset/cached_dev_Bert_Classifier_small_128_cola\n",
            "09_BERT/Test_Dataset/dev.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Va-9Z80pAQr",
        "colab_type": "text"
      },
      "source": [
        "### Set the dataset version\n",
        "The enviroment variable DATA_DIR holds the path to the dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-Bg5I-VrylZ",
        "colab_type": "code",
        "outputId": "165d3295-d141-4f54-ef53-cba62acd1ee0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%env DATA_DIR=./09_BERT/Small_Dataset/\n",
        "\n",
        "#Uncomment the following line to use the large version of the dataset\n",
        "#%env DATA_DIR=./09_BERT/Large_Dataset/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: DATA_DIR=./09_BERT/Small_Dataset/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8WmMvszp3so",
        "colab_type": "text"
      },
      "source": [
        "### Inspect the dataset\n",
        "\n",
        "The dataset is in the tsv format expected by transfomer library. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zymDOvKEcMi5",
        "colab_type": "code",
        "outputId": "a11af165-9570-4772-9f93-7cb3d7c9a634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "test = pd.read_csv(os.environ[\"DATA_DIR\"] + \"dev.tsv\", header=None, sep = '\\t')\n",
        "data = pd.DataFrame(test)\n",
        "data.columns = [\"index\", \"label\", \"mark\", \"tweet\"]\n",
        "data"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>label</th>\n",
              "      <th>mark</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>Now Playing: ♬ Dick Curless - Evil Hearted Me ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>Not only are you comfortably swaddled in secur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>Follow @iAmMySign !!!  Follow @iAmMySign our o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>These strawberry sandwich cookies are so easy ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>Do These Two Lines Match Up On Your Hands Here...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>I’m sorry you hurt your first-grade teacher’s ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>96</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>#HometimeReading: If you’ve enjoyed #KewOrchid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>97</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>Miss_5_Thousand : All my afternoon plans just ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>98</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>A bunch of associates, that I hardly associate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>99</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>Chokehold - Underneath  https://t.co/Vkj2jrIFEb</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    index  label mark                                              tweet\n",
              "0       0      1    a  Now Playing: ♬ Dick Curless - Evil Hearted Me ...\n",
              "1       1      0    a  Not only are you comfortably swaddled in secur...\n",
              "2       2      1    a  Follow @iAmMySign !!!  Follow @iAmMySign our o...\n",
              "3       3      0    a  These strawberry sandwich cookies are so easy ...\n",
              "4       4      0    a  Do These Two Lines Match Up On Your Hands Here...\n",
              "..    ...    ...  ...                                                ...\n",
              "95     95      0    a  I’m sorry you hurt your first-grade teacher’s ...\n",
              "96     96      1    a  #HometimeReading: If you’ve enjoyed #KewOrchid...\n",
              "97     97      1    a  Miss_5_Thousand : All my afternoon plans just ...\n",
              "98     98      0    a  A bunch of associates, that I hardly associate...\n",
              "99     99      1    a    Chokehold - Underneath  https://t.co/Vkj2jrIFEb\n",
              "\n",
              "[100 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pOZytkUVDJt",
        "colab_type": "text"
      },
      "source": [
        "# Hands-on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVRF_BMtOFBK",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization \n",
        "\n",
        "Recent neural languages models use subword representations. ELMO relies on characters, Open AI GPT on byte pair encoding, and BERT on the word pieces algorithms. These **subword representations are combined when unseen words during training needs to be processed, hence avoiding the OOV problem**. \n",
        "\n",
        "BERT uses a 30k WordPieces vocabulary. \n",
        "\n",
        "Let us see how the BERT Tokenizer works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1Hb2SAeqKrE",
        "colab_type": "code",
        "outputId": "37d7be4d-44c4-4476-8263-a84fa0ddca56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "77f1ac19effb4dbab5e416acf5617b7f",
            "e602650934884f489e0eabcb97343c7a",
            "6fa65b848bd54ec58935d0fbe9da8c42",
            "58fae47344ff4ebd983a518fb3f59f53",
            "c6078fa530d942ba9431f6076a4c698e",
            "71c976da80934f05be46e382815d3b1f",
            "39260f42789f46e3926aa159e3dbb303",
            "43c1896c2b7a4201a9384a7830dcc92b"
          ]
        }
      },
      "source": [
        "from transformers import *\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "text = input(\"Enter a word or a sentence: \")\n",
        "print(tokenizer.tokenize(text))\n",
        "print(tokenizer.encode(text))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77f1ac19effb4dbab5e416acf5617b7f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Enter a word or a sentence: ELMO relies on characters, Open AI GPT on byte pair encoding, and BERT on the word pieces algorithms\n",
            "['elm', '##o', 'relies', 'on', 'characters', ',', 'open', 'ai', 'gp', '##t', 'on', 'byte', 'pair', 'encoding', ',', 'and', 'bert', 'on', 'the', 'word', 'pieces', 'algorithms']\n",
            "[101, 17709, 2080, 16803, 2006, 3494, 1010, 2330, 9932, 14246, 2102, 2006, 24880, 3940, 17181, 1010, 1998, 14324, 2006, 1996, 2773, 4109, 13792, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76AcSBFqFyXi",
        "colab_type": "text"
      },
      "source": [
        "## Fine-Tuning the model\n",
        "\n",
        "The next step would be to fine-tune the model.\n",
        "\n",
        "Running the following script you can fine-tune the model and perform evaluation. While doing the evaluation the classification of the tweets on the test set is saved in the predictions.txt file that we will use later.\n",
        "\n",
        "The most relevant parameters of the script are:\n",
        "  - model type: the model that we are going to use, in this case BERT\n",
        "  - model name or path: the name of the model or path storing a specific model.\n",
        "  - task name: the task that we want to perform, in this case CoLA because we want to do classification.\n",
        "  - ouput dir: the directory in which it stores the fine-tuned model.\n",
        "  \n",
        "You can try to change the parameters and see how it affects performance. \n",
        "\n",
        "This process is slow even though we reduced the dataset. You should expect that it takes around 1 minute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE6sLu94qSvR",
        "colab_type": "code",
        "outputId": "6869ba3e-b3c1-4010-fe98-24b519135c0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python ./09_BERT/run_glue.py \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path bert-base-uncased \\\n",
        "    --task_name CoLA \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_lower_case \\\n",
        "    --data_dir \"$DATA_DIR\" \\\n",
        "    --max_seq_length 128 \\\n",
        "    --per_gpu_eval_batch_size=8   \\\n",
        "    --per_gpu_train_batch_size=8   \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 1.0 \\\n",
        "    --save_steps 62500 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --output_dir  ./Bert_Classifier/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-21 14:49:05.408665: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "05/21/2020 14:49:07 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "05/21/2020 14:49:07 - INFO - filelock -   Lock 140659343022064 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "05/21/2020 14:49:07 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpknnw21be\n",
            "Downloading: 100% 433/433 [00:00<00:00, 369kB/s]\n",
            "05/21/2020 14:49:07 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "05/21/2020 14:49:07 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "05/21/2020 14:49:07 - INFO - filelock -   Lock 140659343022064 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "05/21/2020 14:49:07 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "05/21/2020 14:49:07 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "05/21/2020 14:49:07 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "05/21/2020 14:49:07 - INFO - filelock -   Lock 140659342985480 acquired on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "05/21/2020 14:49:07 - INFO - transformers.file_utils -   https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpveyhr1_t\n",
            "Downloading: 100% 440M/440M [00:24<00:00, 18.1MB/s]\n",
            "05/21/2020 14:49:32 - INFO - transformers.file_utils -   storing https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "05/21/2020 14:49:32 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "05/21/2020 14:49:32 - INFO - filelock -   Lock 140659342985480 released on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "05/21/2020 14:49:32 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "05/21/2020 14:49:36 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "05/21/2020 14:49:36 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "05/21/2020 14:49:52 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='./09_BERT/Small_Dataset/', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='./Bert_Classifier/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=62500, seed=42, server_ip='', server_port='', task_name='cola', tokenizer_name='', tpu=False, tpu_ip_address='', tpu_name='', warmup_steps=0, weight_decay=0.0, xrt_tpu_config='')\n",
            "05/21/2020 14:49:52 - INFO - __main__ -   Loading features from cached file ./09_BERT/Small_Dataset/cached_train_bert-base-uncased_128_cola\n",
            "05/21/2020 14:49:52 - INFO - __main__ -   ***** Running training *****\n",
            "05/21/2020 14:49:52 - INFO - __main__ -     Num examples = 1000\n",
            "05/21/2020 14:49:52 - INFO - __main__ -     Num Epochs = 1\n",
            "05/21/2020 14:49:52 - INFO - __main__ -     Instantaneous batch size per GPU = 8\n",
            "05/21/2020 14:49:52 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "05/21/2020 14:49:52 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "05/21/2020 14:49:52 - INFO - __main__ -     Total optimization steps = 125\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/125 [00:00<?, ?it/s]\u001b[A/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n",
            "\n",
            "Iteration:   1% 1/125 [00:00<01:12,  1.71it/s]\u001b[A\n",
            "Iteration:   2% 2/125 [00:00<00:57,  2.13it/s]\u001b[A\n",
            "Iteration:   2% 3/125 [00:00<00:46,  2.63it/s]\u001b[A\n",
            "Iteration:   3% 4/125 [00:01<00:37,  3.21it/s]\u001b[A\n",
            "Iteration:   4% 5/125 [00:01<00:31,  3.83it/s]\u001b[A\n",
            "Iteration:   5% 6/125 [00:01<00:27,  4.35it/s]\u001b[A\n",
            "Iteration:   6% 7/125 [00:01<00:24,  4.90it/s]\u001b[A\n",
            "Iteration:   6% 8/125 [00:01<00:21,  5.36it/s]\u001b[A\n",
            "Iteration:   7% 9/125 [00:01<00:20,  5.74it/s]\u001b[A\n",
            "Iteration:   8% 10/125 [00:02<00:19,  5.91it/s]\u001b[A\n",
            "Iteration:   9% 11/125 [00:02<00:18,  6.17it/s]\u001b[A\n",
            "Iteration:  10% 12/125 [00:02<00:18,  6.26it/s]\u001b[A\n",
            "Iteration:  10% 13/125 [00:02<00:17,  6.44it/s]\u001b[A\n",
            "Iteration:  11% 14/125 [00:02<00:17,  6.50it/s]\u001b[A\n",
            "Iteration:  12% 15/125 [00:02<00:16,  6.62it/s]\u001b[A\n",
            "Iteration:  13% 16/125 [00:02<00:16,  6.63it/s]\u001b[A\n",
            "Iteration:  14% 17/125 [00:03<00:18,  5.72it/s]\u001b[A\n",
            "Iteration:  14% 18/125 [00:03<00:21,  5.07it/s]\u001b[A\n",
            "Iteration:  15% 19/125 [00:03<00:20,  5.10it/s]\u001b[A\n",
            "Iteration:  16% 20/125 [00:03<00:18,  5.54it/s]\u001b[A\n",
            "Iteration:  17% 21/125 [00:03<00:17,  5.83it/s]\u001b[A\n",
            "Iteration:  18% 22/125 [00:04<00:17,  6.03it/s]\u001b[A\n",
            "Iteration:  18% 23/125 [00:04<00:16,  6.25it/s]\u001b[A\n",
            "Iteration:  19% 24/125 [00:04<00:15,  6.45it/s]\u001b[A\n",
            "Iteration:  20% 25/125 [00:04<00:15,  6.35it/s]\u001b[A\n",
            "Iteration:  21% 26/125 [00:04<00:15,  6.53it/s]\u001b[A\n",
            "Iteration:  22% 27/125 [00:04<00:14,  6.64it/s]\u001b[A\n",
            "Iteration:  22% 28/125 [00:04<00:15,  6.45it/s]\u001b[A\n",
            "Iteration:  23% 29/125 [00:05<00:14,  6.52it/s]\u001b[A\n",
            "Iteration:  24% 30/125 [00:05<00:14,  6.59it/s]\u001b[A\n",
            "Iteration:  25% 31/125 [00:05<00:14,  6.62it/s]\u001b[A\n",
            "Iteration:  26% 32/125 [00:05<00:13,  6.64it/s]\u001b[A\n",
            "Iteration:  26% 33/125 [00:05<00:13,  6.66it/s]\u001b[A\n",
            "Iteration:  27% 34/125 [00:05<00:13,  6.65it/s]\u001b[A\n",
            "Iteration:  28% 35/125 [00:05<00:13,  6.75it/s]\u001b[A\n",
            "Iteration:  29% 36/125 [00:06<00:13,  6.77it/s]\u001b[A\n",
            "Iteration:  30% 37/125 [00:06<00:12,  6.84it/s]\u001b[A\n",
            "Iteration:  30% 38/125 [00:06<00:12,  6.70it/s]\u001b[A\n",
            "Iteration:  31% 39/125 [00:06<00:12,  6.77it/s]\u001b[A\n",
            "Iteration:  32% 40/125 [00:06<00:12,  6.77it/s]\u001b[A\n",
            "Iteration:  33% 41/125 [00:06<00:12,  6.57it/s]\u001b[A\n",
            "Iteration:  34% 42/125 [00:07<00:12,  6.53it/s]\u001b[A\n",
            "Iteration:  34% 43/125 [00:07<00:12,  6.61it/s]\u001b[A\n",
            "Iteration:  35% 44/125 [00:07<00:12,  6.65it/s]\u001b[A\n",
            "Iteration:  36% 45/125 [00:07<00:11,  6.74it/s]\u001b[A\n",
            "Iteration:  37% 46/125 [00:07<00:11,  6.82it/s]\u001b[A\n",
            "Iteration:  38% 47/125 [00:07<00:11,  6.84it/s]\u001b[A\n",
            "Iteration:  38% 48/125 [00:07<00:11,  6.55it/s]\u001b[A\n",
            "Iteration:  39% 49/125 [00:08<00:11,  6.59it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "\n",
            "Iteration:  40% 50/125 [00:08<00:11,  6.55it/s]\u001b[A\n",
            "Iteration:  41% 51/125 [00:08<00:11,  6.66it/s]\u001b[A\n",
            "Iteration:  42% 52/125 [00:08<00:11,  6.62it/s]\u001b[A\n",
            "Iteration:  42% 53/125 [00:08<00:10,  6.71it/s]\u001b[A\n",
            "Iteration:  43% 54/125 [00:08<00:10,  6.72it/s]\u001b[A\n",
            "Iteration:  44% 55/125 [00:08<00:10,  6.52it/s]\u001b[A\n",
            "Iteration:  45% 56/125 [00:09<00:10,  6.49it/s]\u001b[A\n",
            "Iteration:  46% 57/125 [00:09<00:10,  6.62it/s]\u001b[A\n",
            "Iteration:  46% 58/125 [00:09<00:10,  6.61it/s]\u001b[A\n",
            "Iteration:  47% 59/125 [00:09<00:09,  6.67it/s]\u001b[A\n",
            "Iteration:  48% 60/125 [00:09<00:09,  6.69it/s]\u001b[A\n",
            "Iteration:  49% 61/125 [00:09<00:09,  6.75it/s]\u001b[A\n",
            "Iteration:  50% 62/125 [00:10<00:09,  6.81it/s]\u001b[A\n",
            "Iteration:  50% 63/125 [00:10<00:09,  6.79it/s]\u001b[A\n",
            "Iteration:  51% 64/125 [00:10<00:08,  6.86it/s]\u001b[A\n",
            "Iteration:  52% 65/125 [00:10<00:08,  6.85it/s]\u001b[A\n",
            "Iteration:  53% 66/125 [00:10<00:08,  6.84it/s]\u001b[A\n",
            "Iteration:  54% 67/125 [00:10<00:08,  6.79it/s]\u001b[A\n",
            "Iteration:  54% 68/125 [00:10<00:08,  6.82it/s]\u001b[A\n",
            "Iteration:  55% 69/125 [00:11<00:08,  6.78it/s]\u001b[A\n",
            "Iteration:  56% 70/125 [00:11<00:08,  6.75it/s]\u001b[A\n",
            "Iteration:  57% 71/125 [00:11<00:08,  6.74it/s]\u001b[A\n",
            "Iteration:  58% 72/125 [00:11<00:07,  6.79it/s]\u001b[A\n",
            "Iteration:  58% 73/125 [00:11<00:07,  6.81it/s]\u001b[A\n",
            "Iteration:  59% 74/125 [00:11<00:07,  6.81it/s]\u001b[A\n",
            "Iteration:  60% 75/125 [00:11<00:07,  6.82it/s]\u001b[A\n",
            "Iteration:  61% 76/125 [00:12<00:07,  6.52it/s]\u001b[A\n",
            "Iteration:  62% 77/125 [00:12<00:07,  6.40it/s]\u001b[A\n",
            "Iteration:  62% 78/125 [00:12<00:07,  6.31it/s]\u001b[A\n",
            "Iteration:  63% 79/125 [00:12<00:07,  6.13it/s]\u001b[A\n",
            "Iteration:  64% 80/125 [00:12<00:07,  6.07it/s]\u001b[A\n",
            "Iteration:  65% 81/125 [00:12<00:07,  6.02it/s]\u001b[A\n",
            "Iteration:  66% 82/125 [00:13<00:07,  6.00it/s]\u001b[A\n",
            "Iteration:  66% 83/125 [00:13<00:07,  5.91it/s]\u001b[A\n",
            "Iteration:  67% 84/125 [00:13<00:06,  5.98it/s]\u001b[A\n",
            "Iteration:  68% 85/125 [00:13<00:06,  5.93it/s]\u001b[A\n",
            "Iteration:  69% 86/125 [00:13<00:06,  5.98it/s]\u001b[A\n",
            "Iteration:  70% 87/125 [00:13<00:06,  6.24it/s]\u001b[A\n",
            "Iteration:  70% 88/125 [00:14<00:05,  6.42it/s]\u001b[A\n",
            "Iteration:  71% 89/125 [00:14<00:05,  6.51it/s]\u001b[A\n",
            "Iteration:  72% 90/125 [00:14<00:05,  6.57it/s]\u001b[A\n",
            "Iteration:  73% 91/125 [00:14<00:05,  6.67it/s]\u001b[A\n",
            "Iteration:  74% 92/125 [00:14<00:04,  6.74it/s]\u001b[A\n",
            "Iteration:  74% 93/125 [00:14<00:04,  6.81it/s]\u001b[A\n",
            "Iteration:  75% 94/125 [00:14<00:04,  6.84it/s]\u001b[A\n",
            "Iteration:  76% 95/125 [00:15<00:04,  6.75it/s]\u001b[A\n",
            "Iteration:  77% 96/125 [00:15<00:04,  6.69it/s]\u001b[A\n",
            "Iteration:  78% 97/125 [00:15<00:04,  6.62it/s]\u001b[A\n",
            "Iteration:  78% 98/125 [00:15<00:04,  6.69it/s]\u001b[A\n",
            "Iteration:  79% 99/125 [00:15<00:03,  6.64it/s]\u001b[A\n",
            "Iteration:  80% 100/125 [00:15<00:03,  6.69it/s]\u001b[A\n",
            "Iteration:  81% 101/125 [00:16<00:03,  6.42it/s]\u001b[A\n",
            "Iteration:  82% 102/125 [00:16<00:03,  6.31it/s]\u001b[A\n",
            "Iteration:  82% 103/125 [00:16<00:03,  6.43it/s]\u001b[A\n",
            "Iteration:  83% 104/125 [00:16<00:03,  6.58it/s]\u001b[A\n",
            "Iteration:  84% 105/125 [00:16<00:03,  6.41it/s]\u001b[A\n",
            "Iteration:  85% 106/125 [00:16<00:02,  6.55it/s]\u001b[A\n",
            "Iteration:  86% 107/125 [00:16<00:02,  6.62it/s]\u001b[A\n",
            "Iteration:  86% 108/125 [00:17<00:02,  6.71it/s]\u001b[A\n",
            "Iteration:  87% 109/125 [00:17<00:02,  6.77it/s]\u001b[A\n",
            "Iteration:  88% 110/125 [00:17<00:02,  6.80it/s]\u001b[A\n",
            "Iteration:  89% 111/125 [00:17<00:02,  6.74it/s]\u001b[A\n",
            "Iteration:  90% 112/125 [00:17<00:01,  6.67it/s]\u001b[A\n",
            "Iteration:  90% 113/125 [00:17<00:01,  6.65it/s]\u001b[A\n",
            "Iteration:  91% 114/125 [00:17<00:01,  6.74it/s]\u001b[A\n",
            "Iteration:  92% 115/125 [00:18<00:01,  6.70it/s]\u001b[A\n",
            "Iteration:  93% 116/125 [00:18<00:01,  6.78it/s]\u001b[A\n",
            "Iteration:  94% 117/125 [00:18<00:01,  6.71it/s]\u001b[A\n",
            "Iteration:  94% 118/125 [00:18<00:01,  6.78it/s]\u001b[A\n",
            "Iteration:  95% 119/125 [00:18<00:00,  6.75it/s]\u001b[A\n",
            "Iteration:  96% 120/125 [00:18<00:00,  6.61it/s]\u001b[A\n",
            "Iteration:  97% 121/125 [00:19<00:00,  6.57it/s]\u001b[A\n",
            "Iteration:  98% 122/125 [00:19<00:00,  6.66it/s]\u001b[A\n",
            "Iteration:  98% 123/125 [00:19<00:00,  6.63it/s]\u001b[A\n",
            "Iteration:  99% 124/125 [00:19<00:00,  6.68it/s]\u001b[A\n",
            "Iteration: 100% 125/125 [00:19<00:00,  6.38it/s]\n",
            "Epoch: 100% 1/1 [00:19<00:00, 19.60s/it]\n",
            "05/21/2020 14:50:12 - INFO - __main__ -    global_step = 125, average loss = 0.6568606090545654\n",
            "05/21/2020 14:50:12 - INFO - __main__ -   Saving model checkpoint to ./Bert_Classifier/\n",
            "05/21/2020 14:50:12 - INFO - transformers.configuration_utils -   Configuration saved in ./Bert_Classifier/config.json\n",
            "05/21/2020 14:50:13 - INFO - transformers.modeling_utils -   Model weights saved in ./Bert_Classifier/pytorch_model.bin\n",
            "05/21/2020 14:50:13 - INFO - transformers.configuration_utils -   loading configuration file ./Bert_Classifier/config.json\n",
            "05/21/2020 14:50:13 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "05/21/2020 14:50:13 - INFO - transformers.modeling_utils -   loading weights file ./Bert_Classifier/pytorch_model.bin\n",
            "05/21/2020 14:50:17 - INFO - transformers.tokenization_utils -   Model name './Bert_Classifier/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming './Bert_Classifier/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "05/21/2020 14:50:17 - INFO - transformers.tokenization_utils -   Didn't find file ./Bert_Classifier/added_tokens.json. We won't load it.\n",
            "05/21/2020 14:50:17 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/vocab.txt\n",
            "05/21/2020 14:50:17 - INFO - transformers.tokenization_utils -   loading file None\n",
            "05/21/2020 14:50:17 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/special_tokens_map.json\n",
            "05/21/2020 14:50:17 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/tokenizer_config.json\n",
            "05/21/2020 14:50:17 - INFO - transformers.tokenization_utils -   Model name './Bert_Classifier/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming './Bert_Classifier/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "05/21/2020 14:50:17 - INFO - transformers.tokenization_utils -   Didn't find file ./Bert_Classifier/added_tokens.json. We won't load it.\n",
            "05/21/2020 14:50:17 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/vocab.txt\n",
            "05/21/2020 14:50:17 - INFO - transformers.tokenization_utils -   loading file None\n",
            "05/21/2020 14:50:17 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/special_tokens_map.json\n",
            "05/21/2020 14:50:17 - INFO - transformers.tokenization_utils -   loading file ./Bert_Classifier/tokenizer_config.json\n",
            "05/21/2020 14:50:17 - INFO - __main__ -   Evaluate the following checkpoints: ['./Bert_Classifier/']\n",
            "05/21/2020 14:50:17 - INFO - transformers.configuration_utils -   loading configuration file ./Bert_Classifier/config.json\n",
            "05/21/2020 14:50:17 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "05/21/2020 14:50:17 - INFO - transformers.modeling_utils -   loading weights file ./Bert_Classifier/pytorch_model.bin\n",
            "05/21/2020 14:50:21 - INFO - __main__ -   Loading features from cached file ./09_BERT/Small_Dataset/cached_dev_bert-base-uncased_128_cola\n",
            "05/21/2020 14:50:21 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "05/21/2020 14:50:21 - INFO - __main__ -     Num examples = 100\n",
            "05/21/2020 14:50:21 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100% 13/13 [00:00<00:00, 28.86it/s]\n",
            "05/21/2020 14:50:22 - INFO - __main__ -   ***** Eval results  *****\n",
            "05/21/2020 14:50:22 - INFO - __main__ -     mcc = 0.24851594087962242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1SZ9fLzHrPM",
        "colab_type": "text"
      },
      "source": [
        "If you trained with the small dataset you should see a final result such as mcc = 0.24.\n",
        "\n",
        "On the other hand if you trained with the large one mcc increases to 0.70\n",
        "\n",
        "The MCC score measures how well does the algorithm perform on both positive and negative predictions.\n",
        "\n",
        "It gives more information than the accuracy or the f1 score.\n",
        "\n",
        "This numbers ranges from -1 to 1 being 0 the random case, -1 the worst value and +1 the best value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUmytTFLWSZp",
        "colab_type": "text"
      },
      "source": [
        "## Further Evaluation\n",
        "\n",
        "Let's compute the metrics of our fine-tuned model to see how well it performs on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au85e90b2Ed6",
        "colab_type": "code",
        "outputId": "1a7f673e-dd46-432b-9648-03a612ed6447",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "preds = np.loadtxt(\"./Bert_Classifier/predictions.txt\")\n",
        "test = pd.read_csv(os.environ[\"DATA_DIR\"] + \"dev.tsv\", header=None, sep = '\\t')\n",
        "\n",
        "print(classification_report(np.asarray(test[1]), preds))\n",
        "print(\"Accuracy: \", accuracy_score(np.asarray(test[1]), preds))\n",
        "print(\"MCC: \", matthews_corrcoef(test[1], preds))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.74      0.68        54\n",
            "           1       0.62      0.50      0.55        46\n",
            "\n",
            "    accuracy                           0.63       100\n",
            "   macro avg       0.63      0.62      0.62       100\n",
            "weighted avg       0.63      0.63      0.62       100\n",
            "\n",
            "Accuracy:  0.63\n",
            "MCC:  0.24851594087962242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1l41weoHzUW",
        "colab_type": "text"
      },
      "source": [
        "You should see an accuracy of 0.63 and an f1-score of 0.62 which is a good result considering the size of the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSGsGLSNFkpr",
        "colab_type": "text"
      },
      "source": [
        "The full model fine-tuned on the 500k tweets achieve the following metrics:\n",
        "\n",
        "    - Accuracy = 0.85\n",
        "    - Recall = 0.85\n",
        "    - Precision = 0.86\n",
        "    - Recall = 0.85\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yIjsNITWfGg",
        "colab_type": "text"
      },
      "source": [
        "## Perform inference\n",
        "\n",
        "Now let's take some random examples from our test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zyHDRIKBSeN",
        "colab_type": "code",
        "outputId": "ef55c125-b0f9-4165-806b-568aeb9ba99f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "os.mkdir(\"./Test_Dataset/\") # We are going to store the test dataset in this folder\n",
        "\n",
        "test_evaluate = test[:4]\n",
        "print(test_evaluate)\n",
        "test_evaluate.to_csv(\"./Test_Dataset/dev.tsv\", sep='\\t', index=False, header=False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   0  1  2                                                  3\n",
            "0  0  1  a  Now Playing: ♬ Dick Curless - Evil Hearted Me ...\n",
            "1  1  0  a  Not only are you comfortably swaddled in secur...\n",
            "2  2  1  a  Follow @iAmMySign !!!  Follow @iAmMySign our o...\n",
            "3  3  0  a  These strawberry sandwich cookies are so easy ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry28Kp1oIZgR",
        "colab_type": "text"
      },
      "source": [
        "If you want to perform inference with the larger model we provide an already trained version. You only have to change the argument in model_name_or path from Bert_Classifier_small to Bert_Classifier_Large"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQHEysl0QQWq",
        "colab_type": "code",
        "outputId": "7a0040f0-675d-49bb-d82c-659e6b1beb22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#%env MODEL_PATH=./Bert_Classifier/\n",
        "\n",
        "#Uncomment the following line to use the version of the model trained with the large dataset\n",
        "%env MODEL_PATH=./09_BERT/Bert_Classifier_Large/"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: MODEL_PATH=./09_BERT/Bert_Classifier_Large/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWOTIq74DqOj",
        "colab_type": "code",
        "outputId": "e2966363-695c-44c7-d775-f11a1de2f8b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python ./09_BERT/run_glue.py \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path \"$MODEL_PATH\" \\\n",
        "    --task_name CoLA \\\n",
        "    --do_eval \\\n",
        "    --do_lower_case \\\n",
        "    --data_dir ./Test_Dataset/ \\\n",
        "    --max_seq_length 128 \\\n",
        "    --per_gpu_eval_batch_size=8   \\\n",
        "    --per_gpu_train_batch_size=8   \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 1.0 \\\n",
        "    --save_steps 62500 \\\n",
        "    --output_dir  \"$MODEL_PATH\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-21 14:52:49.026841: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "05/21/2020 14:52:50 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "05/21/2020 14:52:50 - INFO - transformers.configuration_utils -   loading configuration file ./09_BERT/Bert_Classifier_Large/config.json\n",
            "05/21/2020 14:52:50 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "05/21/2020 14:52:50 - INFO - transformers.tokenization_utils -   Model name './09_BERT/Bert_Classifier_Large/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming './09_BERT/Bert_Classifier_Large/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "05/21/2020 14:52:50 - INFO - transformers.tokenization_utils -   loading file ./09_BERT/Bert_Classifier_Large/vocab.txt\n",
            "05/21/2020 14:52:50 - INFO - transformers.tokenization_utils -   loading file ./09_BERT/Bert_Classifier_Large/added_tokens.json\n",
            "05/21/2020 14:52:50 - INFO - transformers.tokenization_utils -   loading file ./09_BERT/Bert_Classifier_Large/special_tokens_map.json\n",
            "05/21/2020 14:52:50 - INFO - transformers.tokenization_utils -   loading file ./09_BERT/Bert_Classifier_Large/tokenizer_config.json\n",
            "05/21/2020 14:52:50 - INFO - transformers.modeling_utils -   loading weights file ./09_BERT/Bert_Classifier_Large/pytorch_model.bin\n",
            "05/21/2020 14:52:58 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='./Test_Dataset/', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='./09_BERT/Bert_Classifier_Large/', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='./09_BERT/Bert_Classifier_Large/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=62500, seed=42, server_ip='', server_port='', task_name='cola', tokenizer_name='', tpu=False, tpu_ip_address='', tpu_name='', warmup_steps=0, weight_decay=0.0, xrt_tpu_config='')\n",
            "05/21/2020 14:52:58 - INFO - transformers.tokenization_utils -   Model name './09_BERT/Bert_Classifier_Large/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming './09_BERT/Bert_Classifier_Large/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "05/21/2020 14:52:58 - INFO - transformers.tokenization_utils -   loading file ./09_BERT/Bert_Classifier_Large/vocab.txt\n",
            "05/21/2020 14:52:58 - INFO - transformers.tokenization_utils -   loading file ./09_BERT/Bert_Classifier_Large/added_tokens.json\n",
            "05/21/2020 14:52:58 - INFO - transformers.tokenization_utils -   loading file ./09_BERT/Bert_Classifier_Large/special_tokens_map.json\n",
            "05/21/2020 14:52:58 - INFO - transformers.tokenization_utils -   loading file ./09_BERT/Bert_Classifier_Large/tokenizer_config.json\n",
            "05/21/2020 14:52:58 - INFO - __main__ -   Evaluate the following checkpoints: ['./09_BERT/Bert_Classifier_Large/']\n",
            "05/21/2020 14:52:58 - INFO - transformers.configuration_utils -   loading configuration file ./09_BERT/Bert_Classifier_Large/config.json\n",
            "05/21/2020 14:52:58 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "05/21/2020 14:52:58 - INFO - transformers.modeling_utils -   loading weights file ./09_BERT/Bert_Classifier_Large/pytorch_model.bin\n",
            "05/21/2020 14:53:02 - INFO - __main__ -   Creating features from dataset file at ./Test_Dataset/\n",
            "05/21/2020 14:53:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/21/2020 14:53:02 - INFO - transformers.data.processors.glue -   guid: dev-0\n",
            "05/21/2020 14:53:02 - INFO - transformers.data.processors.glue -   features: InputFeatures(input_ids=[101, 2085, 2652, 1024, 100, 5980, 15390, 7971, 1011, 4763, 18627, 2033, 100, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1042, 2480, 21600, 2683, 4313, 2102, 2475, 2232, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "05/21/2020 14:53:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/21/2020 14:53:02 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "05/21/2020 14:53:02 - INFO - transformers.data.processors.glue -   features: InputFeatures(input_ids=[101, 2025, 2069, 2024, 2017, 18579, 25430, 4215, 20043, 1999, 3036, 2651, 1010, 2009, 1521, 1055, 1012, 1012, 1012, 2062, 2005, 6178, 7277, 9691, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 19842, 15721, 2072, 2549, 2290, 2487, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
            "05/21/2020 14:53:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/21/2020 14:53:02 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "05/21/2020 14:53:02 - INFO - transformers.data.processors.glue -   features: InputFeatures(input_ids=[101, 3582, 1030, 24264, 18879, 5332, 16206, 999, 999, 999, 3582, 1030, 24264, 18879, 5332, 16206, 2256, 2880, 3931, 2005, 1996, 2878, 28501, 1012, 3582, 1030, 24264, 18879, 5332, 16206, 999, 999, 999, 3582, 1030, 24264, 18879, 5332, 16206, 999, 999, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "05/21/2020 14:53:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/21/2020 14:53:02 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "05/21/2020 14:53:02 - INFO - transformers.data.processors.glue -   features: InputFeatures(input_ids=[101, 2122, 16876, 11642, 16324, 2024, 2061, 3733, 2000, 2191, 1998, 2061, 11937, 21756, 999, 3819, 2005, 1001, 10756, 10259, 16770, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 1057, 4160, 2581, 3597, 2953, 2475, 2100, 2581, 3081, 1030, 24264, 20492, 28433, 8159, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
            "05/21/2020 14:53:02 - INFO - __main__ -   Saving features into cached file ./Test_Dataset/cached_dev_Bert_Classifier_Large_128_cola\n",
            "05/21/2020 14:53:02 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "05/21/2020 14:53:02 - INFO - __main__ -     Num examples = 4\n",
            "05/21/2020 14:53:02 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100% 1/1 [00:00<00:00, 39.68it/s]\n",
            "05/21/2020 14:53:02 - INFO - __main__ -   ***** Eval results  *****\n",
            "05/21/2020 14:53:02 - INFO - __main__ -     mcc = 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC6oati6H-cx",
        "colab_type": "text"
      },
      "source": [
        "mcc\n",
        "\n",
        "Let's see if the model has correctly classified the examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7chkHgKMECc1",
        "colab_type": "code",
        "outputId": "4efb8027-8e43-4d8f-cfae-4fbbcd05a875",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import os\n",
        "\n",
        "results = np.loadtxt(os.environ['MODEL_PATH'] + \"predictions.txt\")\n",
        "for i,t in enumerate(test_evaluate[3]):\n",
        "    print(t + \" --> \", \"BOT\" if results[i]> 0.5 else \"NOT A BOT\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now Playing: ♬ Dick Curless - Evil Hearted Me ♬ https://t.co/fzgP9IRt2h -->  BOT\n",
            "Not only are you comfortably swaddled in security today, it’s ... More for Capricorn https://t.co/MVCHEli4g1 -->  NOT A BOT\n",
            "Follow @iAmMySign !!!  Follow @iAmMySign our official page for the whole Zodiac.  Follow @iAmMySign !!!  Follow @iAmMySign !!! -->  BOT\n",
            "These strawberry sandwich cookies are so easy to make and so tasty! Perfect for #MothersDay https://t.co/Uq7cooR2y7 via @iamthemaven -->  NOT A BOT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUX8iM5tFZvn",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}